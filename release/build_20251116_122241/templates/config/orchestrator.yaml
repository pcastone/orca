# Orchestrator Configuration Template
# This file configures the Orca orchestrator for distributed task execution
# Copy and customize for your deployment

orchestrator:
  # Planning LLM Configuration
  # The planning LLM breaks down complex tasks into executable steps
  planning_llm:
    provider: "local"  # Options: "local", "remote"
    model: "deepseek-r1"  # Recommended for planning: deepseek-r1, o1, claude-3.5-sonnet
    base_url: "http://localhost:11434"  # For local providers (Ollama, llama.cpp)

    # LLM parameters
    temperature: 0.3  # Lower = more deterministic planning
    max_tokens: 4000  # Enough for detailed plans
    timeout_ms: 30000  # 30 seconds

    # Optional: API key for remote providers
    # api_key: "${ANTHROPIC_API_KEY}"  # Use env var expansion

    # Optional: Custom system prompt path
    # system_prompt_file: "templates/prompts/planning_llm_system.txt"

  # Execution LLM Configuration
  # The execution LLM converts task steps into precise tool calls
  execution_llm:
    provider: "local"  # Options: "local", "remote"
    model: "llama3"  # Recommended: llama3, gpt-4, claude-3-haiku
    base_url: "http://localhost:11434"

    # LLM parameters
    temperature: 0.1  # Very deterministic for precise tool calls
    max_tokens: 2000  # Tool calls are typically short
    timeout_ms: 10000  # 10 seconds

    # Optional: API key for remote providers
    # api_key: "${OPENAI_API_KEY}"

    # Optional: Custom system prompt path
    # system_prompt_file: "templates/prompts/execution_llm_system.txt"

  # Action Interpreter Settings
  # Controls how tasks are interpreted and executed
  interpreter:
    max_retries: 3  # Retry failed tool executions
    retry_delay_ms: 1000  # Delay between retries

    # Context Management
    # Prevents context window overflow for long-running workflows
    context:
      max_tokens: 8000  # Maximum context size
      compression_strategy: "keep_ends"  # Options: "keep_ends", "summarize", "truncate"
      keep_first_tasks: 2  # Keep first N tasks for context
      keep_last_tasks: 3  # Keep last N tasks for recency

    # Result Formatting
    # How tool results are presented back to the LLM
    formatting:
      max_content_length: 2000  # Truncate long outputs
      truncate_large_outputs: true  # Enable truncation
      include_reasoning: false  # Include thinking/reasoning in output

  # WebSocket Client Configuration
  # For connecting to the ACO tool runtime server
  aco_client:
    url: "ws://localhost:8080"  # ACO WebSocket endpoint
    reconnect_attempts: 5  # Number of reconnection attempts
    reconnect_delay_ms: 2000  # Delay between reconnect attempts
    request_timeout_ms: 60000  # Timeout for tool execution requests (60s)

    # Optional: Authentication
    # auth_token: "${ACO_AUTH_TOKEN}"

  # Database Configuration
  # SQLite database for task and workflow persistence
  database:
    path: "~/.orca/orchestrator.db"  # Database file location
    max_connections: 10  # Connection pool size

    # Migration settings
    auto_migrate: true  # Run migrations on startup
    migration_path: "src/crates/orchestrator/migrations"

  # Logging Configuration
  logging:
    level: "info"  # Options: "trace", "debug", "info", "warn", "error"
    format: "json"  # Options: "json", "pretty"
    output: "stdout"  # Options: "stdout", "file"

    # Optional: File logging
    # file_path: "~/.orca/logs/orchestrator.log"
    # max_file_size_mb: 100
    # max_files: 10

  # Performance Tuning
  performance:
    # Task execution
    max_concurrent_tasks: 5  # Maximum parallel task execution
    task_queue_size: 100  # Task queue capacity

    # Workflow execution
    max_concurrent_workflows: 3  # Maximum parallel workflows
    workflow_queue_size: 50  # Workflow queue capacity

    # Checkpointing
    checkpoint_interval_ms: 5000  # Save checkpoints every 5 seconds
    enable_streaming_checkpoints: true  # Stream checkpoint updates

  # Feature Flags
  features:
    enable_replanning: true  # Allow plan-execute workflows to replan
    enable_reflection: true  # Enable reflection pattern
    enable_tool_validation: true  # Validate tool calls before execution
    enable_cost_tracking: true  # Track LLM API costs

# Environment-specific overrides
# Uncomment and customize for different environments

# development:
#   orchestrator:
#     planning_llm:
#       temperature: 0.5  # More creative in dev
#     logging:
#       level: "debug"  # Verbose logging
#     performance:
#       max_concurrent_tasks: 2  # Limit resources

# production:
#   orchestrator:
#     planning_llm:
#       provider: "remote"
#       model: "claude-3.5-sonnet"
#       api_key: "${ANTHROPIC_API_KEY}"
#     execution_llm:
#       provider: "remote"
#       model: "gpt-4"
#       api_key: "${OPENAI_API_KEY}"
#     logging:
#       level: "warn"
#       format: "json"
#       output: "file"
#       file_path: "/var/log/orca/orchestrator.log"
#     database:
#       path: "/var/lib/orca/orchestrator.db"
