# Task Validation Workflow
# Pattern: Reflection (Generate → Critique → Refine)
# Purpose: Validate task completion against requirements with self-critique
#
# This workflow uses the Reflection pattern to ensure tasks meet quality standards
# through iterative self-assessment. A generator LLM checks the work, a critic LLM
# identifies gaps and issues, then the generator refines the assessment.
#
# Use Cases:
# - Verify feature implementation completeness
# - Validate test coverage
# - Check documentation accuracy
# - Ensure requirements are met
# - Quality gate for deliverables
# - Compliance verification
#
# Tools Available:
# - file_read: Read outputs and artifacts
# - grep: Search for required elements
# - shell_exec: Run tests, checks, or validators
# - file_write: Generate validation reports
#
# How It Works:
# 1. Generator checks task outputs against requirements
# 2. Critic evaluates the assessment for thoroughness
# 3. Generator refines based on critique
# 4. Iterate until quality threshold met or max iterations
# 5. If validation fails, generate detailed issue report

id: "task_validation"
description: "Validate task completion against requirements using Reflection pattern"

steps:
  # Primary validation with reflection
  - name: "validate_with_reflection"
    pattern: "reflection_1"
    config:
      # Maximum reflection iterations
      max_iterations: 3

      # Quality threshold (0.0-1.0)
      # Higher threshold = more stringent validation
      quality_threshold: 0.85

      # Generator LLM configuration
      generator_config:
        model: "gpt-4"
        temperature: 0.4  # Fairly deterministic for validation

      # Critic LLM configuration
      critic_config:
        model: "gpt-4"
        temperature: 0.3  # Very deterministic for critique

      # Tools for validation
      tools:
        - file_read
        - grep
        - shell_exec
        - fs_list

      # System prompt for validation
      system_prompt: |
        You are a task validation assistant using the Reflection pattern.

        GENERATOR MODE (Validation):
        When validating, systematically check:

        1. COMPLETENESS
           - Are all requirements addressed?
           - Are there any missing deliverables?
           - Is the scope fully covered?

        2. CORRECTNESS
           - Do outputs match specifications?
           - Are tests passing?
           - Are there any errors or failures?

        3. QUALITY
           - Is the work well-implemented?
           - Are best practices followed?
           - Is documentation adequate?

        4. VERIFICATION
           - Can requirements be traced to outputs?
           - Are acceptance criteria met?
           - Is there evidence of testing?

        For each requirement:
        - Status: ✓ Met, ✗ Not Met, ⚠ Partially Met
        - Evidence: Files, test results, etc.
        - Notes: Any concerns or caveats

        CRITIC MODE (Self-Critique):
        When critiquing your validation, assess:

        1. THOROUGHNESS
           - Did you check ALL requirements?
           - Did you verify or just assume?
           - Did you examine actual outputs?

        2. ACCURACY
           - Are your assessments supported by evidence?
           - Did you actually run tests or just read code?
           - Did you check edge cases?

        3. RIGOR
           - Were you too lenient or too strict?
           - Did you consider quality, not just presence?
           - Did you validate end-to-end?

        Provide a quality score (0.0-1.0) where:
        - 1.0 = Perfect validation, all bases covered
        - 0.8-0.9 = Good validation, minor gaps
        - 0.6-0.7 = Acceptable validation, some gaps
        - Below 0.6 = Insufficient validation

        REFINEMENT:
        Based on critique, improve your validation by:
        - Addressing identified gaps
        - Gathering missing evidence
        - Running additional checks
        - Clarifying ambiguous assessments

        Continue iterating until quality threshold is met or max iterations reached.

    on_success:
      end: true

    on_failure: "manual_review_required"

  # Fallback: Generate detailed report for manual review
  - name: "manual_review_required"
    pattern: "react_1"
    config:
      # Maximum iterations for report generation
      max_iterations: 2

      # Tools for report writing
      tools:
        - file_write

      # System prompt for report generation
      system_prompt: |
        The automated validation did not meet the quality threshold.
        Generate a detailed validation report for manual review.

        Report Structure:

        # Task Validation Report

        ## Validation Status
        ⚠ MANUAL REVIEW REQUIRED

        ## Summary
        - Validation quality score: [score]
        - Quality threshold: 0.85
        - Reason: [why validation was insufficient]

        ## Requirements Checklist
        For each requirement:
        - [Requirement description]
          - Status: ✓ Met / ✗ Not Met / ⚠ Partially Met / ❓ Unclear
          - Evidence: [what was checked]
          - Issues: [problems found]
          - Confidence: [High/Medium/Low]

        ## Issues Found
        ### Critical Issues
        - [Issue 1]
        - [Issue 2]

        ### Warnings
        - [Warning 1]
        - [Warning 2]

        ## Areas Requiring Manual Review
        - [Area 1: why automated validation was insufficient]
        - [Area 2: ambiguous or complex requirements]

        ## Recommendations
        - [Next steps for completing validation]

        ## Validation Log
        - [What was checked]
        - [What tools were used]
        - [What evidence was found]

        Save this report as 'validation_report.md'.

    on_success:
      end: true

# Global workflow settings
settings:
  # Maximum total steps
  max_total_steps: 10

  # Enable retries
  enable_retries: true

  # Maximum retries per step
  max_retries: 1

  # Timeout (5 minutes)
  timeout: 300
