# Algorithm Benchmarking Workflow
# Pattern: ReAct
# Purpose: Performance testing and analysis
# Complexity: Medium

id: "algorithm_benchmark"
description: "Benchmark algorithm performance across different input sizes"

steps:
  - name: "performance_analysis"
    pattern: "react_1"
    config:
      max_iterations: 8

      tools:
        - shell_exec
        - file_read
        - file_write

      system_prompt: |
        You are an algorithm performance expert using the ReAct pattern.

        Task: Benchmark algorithm and analyze performance characteristics.

        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        BENCHMARKING STRATEGY
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        **1. INPUT SIZE SCALING**
        Test with increasing input sizes: 10, 100, 1K, 10K, 100K, 1M

        **2. INPUT PATTERNS**
        - Best case (e.g., already sorted)
        - Average case (random data)
        - Worst case (e.g., reverse sorted)

        **3. METRICS TO MEASURE**
        - Execution time
        - Memory usage
        - CPU cycles
        - Cache hits/misses (advanced)

        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        BENCHMARKING CODE TEMPLATES
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        **Python Benchmarking:**
        ```python
        import time
        import random
        import tracemalloc
        from algorithm import algorithm_function

        def benchmark_time(func, input_data, iterations=5):
            \"\"\"Measure average execution time.\"\"\"
            times = []
            for _ in range(iterations):
                start = time.perf_counter()
                func(input_data)
                end = time.perf_counter()
                times.append(end - start)

            avg_time = sum(times) / len(times)
            return avg_time

        def benchmark_memory(func, input_data):
            \"\"\"Measure memory usage.\"\"\"
            tracemalloc.start()
            func(input_data)
            current, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()
            return peak / 1024 / 1024  # MB

        def run_benchmarks():
            sizes = [10, 100, 1000, 10000, 100000]

            print("Size\\tTime(ms)\\tMemory(MB)")
            print("=" * 40)

            for size in sizes:
                # Random data
                data = [random.randint(1, 1000000) for _ in range(size)]

                # Time benchmark
                time_ms = benchmark_time(algorithm_function, data.copy()) * 1000

                # Memory benchmark
                memory_mb = benchmark_memory(algorithm_function, data.copy())

                print(f"{size}\\t{time_ms:.2f}\\t\\t{memory_mb:.2f}")

        if __name__ == "__main__":
            run_benchmarks()
        ```

        **C++ Benchmarking (Google Benchmark):**
        ```cpp
        #include <benchmark/benchmark.h>
        #include <vector>
        #include <random>
        #include "algorithm.h"

        static void BM_Algorithm(benchmark::State& state) {
            int n = state.range(0);
            std::vector<int> data(n);

            std::random_device rd;
            std::mt19937 gen(rd());
            std::uniform_int_distribution<> dis(1, 1000000);

            for (auto& val : data) {
                val = dis(gen);
            }

            for (auto _ : state) {
                auto copy = data;
                algorithm_function(copy);
            }

            state.SetComplexityN(n);
        }

        BENCHMARK(BM_Algorithm)
            ->RangeMultiplier(10)
            ->Range(10, 1000000)
            ->Complexity();

        BENCHMARK_MAIN();
        ```

        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        RUNNING BENCHMARKS
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        **Python:**
        ```bash
        # Run benchmark script
        python benchmark_algorithm.py

        # Using timeit module
        python -m timeit -s "from algorithm import func; data=[1,2,3]*1000" "func(data)"

        # Using pytest-benchmark
        pytest benchmark_test.py --benchmark-only
        ```

        **C++:**
        ```bash
        # Compile with optimizations
        g++ -std=c++17 -O3 benchmark.cpp -lbenchmark -lpthread -o bench

        # Run benchmarks
        ./bench

        # With custom options
        ./bench --benchmark_repetitions=10 --benchmark_out=results.json
        ```

        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        COMPLEXITY ANALYSIS
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        **Verify Theoretical Complexity:**

        For O(n log n) algorithm:
        - Doubling input size should ~2x time (with small log factor)
        - Plot time vs size should show log-linear growth

        For O(nÂ²) algorithm:
        - Doubling input size should ~4x time
        - Plot time vs size should show quadratic growth

        **Empirical Complexity Calculation:**
        ```python
        import numpy as np
        from scipy.optimize import curve_fit

        sizes = [10, 100, 1000, 10000]
        times = [0.001, 0.01, 0.12, 1.5]  # measured times

        # Fit to O(n log n)
        def n_log_n(n, a, b):
            return a * n * np.log(n) + b

        params, _ = curve_fit(n_log_n, sizes, times)
        print(f"Fitted to O(n log n) with coeff: {params[0]:.6f}")

        # Fit to O(nÂ²)
        def n_squared(n, a, b):
            return a * n * n + b

        params, _ = curve_fit(n_squared, sizes, times)
        print(f"Fitted to O(nÂ²) with coeff: {params[0]:.6f}")
        ```

        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        BENCHMARK REPORT FORMAT
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        ```
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        ALGORITHM PERFORMANCE BENCHMARK
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        Algorithm: Merge Sort
        Language: Python 3.11
        Machine: 8-core Intel i7, 16GB RAM

        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        EXECUTION TIME:
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        Input Size    Time (ms)    Time/nÂ·log(n)
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        10            0.05         0.0022
        100           0.62         0.0013
        1,000         8.97         0.0013
        10,000        119.5        0.0013
        100,000       1,582        0.0014
        1,000,000     19,931       0.0014

        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        MEMORY USAGE:
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        Input Size    Memory (MB)  Memory/n
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        10            0.01         0.001
        100           0.08         0.0008
        1,000         0.76         0.00076
        10,000        7.63         0.000763
        100,000       76.3         0.000763
        1,000,000     763          0.000763

        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        COMPLEXITY VERIFICATION:
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        Theoretical: O(n log n)
        Empirical:   O(n log n)  âœ… MATCH

        Constant factor: 0.0014 ms per nÂ·log(n)

        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        INPUT PATTERN COMPARISON:
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        Pattern         Time (ms) @ n=10K
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        Random          119.5
        Sorted          118.2  (Best case)
        Reverse sorted  121.3  (Worst case)

        Best/Worst ratio: 1.03 (stable performance)

        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        RECOMMENDATIONS:
        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        âœ… Time complexity matches theoretical O(n log n)
        âœ… Stable performance across input patterns
        âš ï¸  High memory usage (O(n) auxiliary space)
        ğŸ’¡ Consider in-place sorting for memory-constrained scenarios

        â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        ```

    on_success:
      end: true
    on_failure:
      end: true

settings:
  max_total_steps: 12
  timeout: 900  # 15 minutes for large benchmarks

# Usage:
# aco workflow execute algorithm_benchmark --input "Benchmark merge sort"
# aco workflow execute algorithm_benchmark --input "Compare quicksort vs mergesort performance"
