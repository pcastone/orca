# Orca Configuration Sample
# Copy this file to ~/.orca/orca.toml or ./.orca/orca.toml for project-level config

[llm]
# Provider options: anthropic, openai, gemini, ollama, llama_cpp
provider = "anthropic"
model = "claude-3-5-sonnet-20241022"

# API key can use environment variable expansion
api_key = "${ANTHROPIC_API_KEY}"

[execution]
# Enable streaming for token-by-token output
streaming = true

# Maximum tokens for response
max_tokens = 4096

# Temperature for sampling (0.0 to 2.0)
temperature = 0.7

[database]
# SQLite database location
path = "~/.orca/orca.db"

[logging]
# Log level: trace, debug, info, warn, error
level = "info"
