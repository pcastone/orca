//! Response types from LLM chat completions.
//!
//! This module defines the types returned by `ChatModel::chat()` and
//! `ChatModel::stream()`, including support for thinking models, token
//! usage tracking, and streaming responses.

use crate::llm_stream::MessageChunkStream;
use crate::Message;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;

/// Response from a chat completion request.
///
/// Contains the model's response message along with metadata about token usage,
/// reasoning (for thinking models), and provider-specific information.
///
/// # Example
///
/// ```rust,ignore
/// let response = model.chat(request).await?;
///
/// // Access the response message
/// println!("Answer: {}", response.message.text());
///
/// // Check token usage
/// if let Some(usage) = response.usage {
///     println!("Tokens used: {}", usage.total_tokens);
/// }
///
/// // Access reasoning for thinking models
/// if let Some(reasoning) = response.reasoning {
///     println!("Model thought: {}", reasoning.content);
/// }
/// ```
#[derive(Debug, Clone)]
pub struct ChatResponse {
    /// The assistant's response message.
    ///
    /// Contains the actual content generated by the model. May include
    /// tool calls if the model decided to use functions.
    pub message: Message,

    /// Token usage statistics for this request.
    ///
    /// Includes input tokens, output tokens, and reasoning tokens (for
    /// thinking models). Providers may not report all fields.
    pub usage: Option<UsageMetadata>,

    /// Extended reasoning content from thinking models.
    ///
    /// Only populated when:
    /// 1. The model supports thinking/reasoning (o1, DeepSeek R1, etc.)
    /// 2. Request used `ReasoningMode::Separated` or `ReasoningMode::Extended`
    ///
    /// The reasoning content shows the model's "thought process" before
    /// generating the final answer in `message.content`.
    pub reasoning: Option<ReasoningContent>,

    /// Provider-specific metadata.
    ///
    /// Implementations can include additional information here:
    /// - Model version/name
    /// - Response ID
    /// - Finish reason (stop, length, tool_calls, etc.)
    /// - Provider-specific flags
    pub metadata: HashMap<String, serde_json::Value>,
}

/// Response from a streaming chat completion request.
///
/// Provides token-by-token streaming of the model's output, optionally
/// including a separate stream for reasoning content.
///
/// # Example
///
/// ```rust,ignore
/// use futures::StreamExt;
///
/// let stream_response = model.stream(request).await?;
///
/// // Stream the main response
/// while let Some(chunk) = stream_response.stream.next().await {
///     print!("{}", chunk.content);
///     std::io::stdout().flush().unwrap();
/// }
///
/// // Check final usage
/// if let Some(usage) = stream_response.usage {
///     println!("\nTokens: {}", usage.total_tokens);
/// }
/// ```
pub struct ChatStreamResponse {
    /// Stream of message chunks.
    ///
    /// Yields `MessageChunk` items containing incremental content updates.
    /// The final chunk has `is_final = true`.
    pub stream: MessageChunkStream,

    /// Optional stream of reasoning chunks (for thinking models).
    ///
    /// Only present when:
    /// 1. Model supports reasoning
    /// 2. Request used `ReasoningMode::Separated` or `ReasoningMode::Extended`
    ///
    /// Streams the model's thinking process in parallel with the main stream.
    pub reasoning_stream: Option<MessageChunkStream>,

    /// Token usage statistics (available after stream completes).
    ///
    /// Note: This field is typically `None` until the stream finishes.
    /// Some providers may update it progressively.
    pub usage: Option<UsageMetadata>,

    /// Provider-specific metadata.
    pub metadata: HashMap<String, serde_json::Value>,
}

/// Token usage statistics for a chat completion.
///
/// Tracks the number of tokens consumed by different parts of the request
/// and response. Thinking models include separate reasoning token counts.
///
/// # Token Counting
///
/// Different providers count tokens differently:
/// - **Input tokens**: Tokens in the request messages (prompts, context)
/// - **Output tokens**: Tokens in the response (assistant message)
/// - **Reasoning tokens**: Tokens used for thinking (thinking models only)
/// - **Total tokens**: Sum of all token types
///
/// # Example
///
/// ```rust,ignore
/// if let Some(usage) = response.usage {
///     println!("Input: {} tokens", usage.input_tokens);
///     println!("Output: {} tokens", usage.output_tokens);
///
///     if let Some(reasoning) = usage.reasoning_tokens {
///         println!("Reasoning: {} tokens", reasoning);
///     }
///
///     println!("Total: {} tokens", usage.total_tokens);
/// }
/// ```
#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
pub struct UsageMetadata {
    /// Number of tokens in the input/prompt.
    ///
    /// Includes all messages sent to the model: system, user, assistant history.
    pub input_tokens: usize,

    /// Number of tokens in the output/completion.
    ///
    /// The actual response content generated by the model.
    pub output_tokens: usize,

    /// Number of tokens used for reasoning/thinking.
    ///
    /// Only applicable to thinking models (o1, DeepSeek R1, etc.).
    /// Represents tokens used during the internal "thought process."
    ///
    /// For standard models, this is `None`.
    pub reasoning_tokens: Option<usize>,

    /// Total tokens consumed (input + output + reasoning).
    ///
    /// Used for billing and quota tracking.
    pub total_tokens: usize,
}

impl UsageMetadata {
    /// Create a new usage metadata instance.
    ///
    /// Automatically calculates `total_tokens` from provided values.
    pub fn new(input_tokens: usize, output_tokens: usize) -> Self {
        Self {
            input_tokens,
            output_tokens,
            reasoning_tokens: None,
            total_tokens: input_tokens + output_tokens,
        }
    }

    /// Create usage metadata with reasoning tokens.
    ///
    /// For thinking models that report separate reasoning token counts.
    pub fn with_reasoning(
        input_tokens: usize,
        output_tokens: usize,
        reasoning_tokens: usize,
    ) -> Self {
        Self {
            input_tokens,
            output_tokens,
            reasoning_tokens: Some(reasoning_tokens),
            total_tokens: input_tokens + output_tokens + reasoning_tokens,
        }
    }
}

/// Extended reasoning/thinking content from capable models.
///
/// Thinking models (OpenAI o1, DeepSeek R1, etc.) perform extended reasoning
/// before generating a final answer. This type captures that reasoning process.
///
/// # When This Is Populated
///
/// Only when BOTH conditions are met:
/// 1. Model supports thinking/reasoning
/// 2. Request used `ReasoningMode::Separated` or `ReasoningMode::Extended`
///
/// # Example
///
/// ```rust,ignore
/// let request = ChatRequest::new(messages)
///     .with_reasoning(ReasoningMode::Separated);
///
/// let response = model.chat(request).await?;
///
/// if let Some(reasoning) = response.reasoning {
///     println!("=== Model's Thinking Process ===");
///     println!("{}", reasoning.content);
///     println!("Reasoning tokens: {}", reasoning.tokens);
///
///     if let Some(duration) = reasoning.duration_ms {
///         println!("Thinking time: {}ms", duration);
///     }
/// }
///
/// println!("\n=== Final Answer ===");
/// println!("{}", response.message.text());
/// ```
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ReasoningContent {
    /// The reasoning/thinking text.
    ///
    /// Contains the model's internal thought process, often showing:
    /// - Problem analysis
    /// - Step-by-step reasoning
    /// - Consideration of alternatives
    /// - Self-correction
    ///
    /// Format varies by model:
    /// - OpenAI o1: Structured reasoning steps
    /// - DeepSeek R1: Text within `<think>` tags (may be pre-parsed)
    pub content: String,

    /// Number of tokens used for reasoning.
    ///
    /// Providers may not report this separately. If unavailable, may be:
    /// - 0 (unknown)
    /// - Estimated from content length
    /// - Included in `UsageMetadata::reasoning_tokens`
    pub tokens: usize,

    /// Time spent on reasoning in milliseconds.
    ///
    /// Not all providers report this. Useful for understanding:
    /// - Model performance
    /// - Extended reasoning vs quick thinking
    /// - Cost/latency tradeoffs
    pub duration_ms: Option<u64>,

    /// Provider-specific reasoning metadata.
    ///
    /// May include:
    /// - Confidence scores
    /// - Reasoning depth level
    /// - Number of reasoning steps
    /// - Model version used for reasoning
    pub metadata: Option<HashMap<String, serde_json::Value>>,
}

impl ReasoningContent {
    /// Create new reasoning content with just the text.
    pub fn new(content: impl Into<String>) -> Self {
        Self {
            content: content.into(),
            tokens: 0,
            duration_ms: None,
            metadata: None,
        }
    }

    /// Set the token count for this reasoning.
    pub fn with_tokens(mut self, tokens: usize) -> Self {
        self.tokens = tokens;
        self
    }

    /// Set the duration for this reasoning.
    pub fn with_duration_ms(mut self, duration_ms: u64) -> Self {
        self.duration_ms = Some(duration_ms);
        self
    }

    /// Add metadata to this reasoning.
    pub fn with_metadata(mut self, metadata: HashMap<String, serde_json::Value>) -> Self {
        self.metadata = Some(metadata);
        self
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_usage_metadata_new() {
        let usage = UsageMetadata::new(100, 50);
        assert_eq!(usage.input_tokens, 100);
        assert_eq!(usage.output_tokens, 50);
        assert_eq!(usage.reasoning_tokens, None);
        assert_eq!(usage.total_tokens, 150);
    }

    #[test]
    fn test_usage_metadata_with_reasoning() {
        let usage = UsageMetadata::with_reasoning(100, 50, 200);
        assert_eq!(usage.input_tokens, 100);
        assert_eq!(usage.output_tokens, 50);
        assert_eq!(usage.reasoning_tokens, Some(200));
        assert_eq!(usage.total_tokens, 350);
    }

    #[test]
    fn test_reasoning_content_builder() {
        let reasoning = ReasoningContent::new("thinking...")
            .with_tokens(150)
            .with_duration_ms(1500);

        assert_eq!(reasoning.content, "thinking...");
        assert_eq!(reasoning.tokens, 150);
        assert_eq!(reasoning.duration_ms, Some(1500));
    }
}
