//! LLM-based Pattern Planner
//!
//! Uses an LLM to generate execution plans for patterns based on user intent
//! and pattern templates.

use crate::{OrchestratorError, Result};
use langgraph_core::llm::{ChatModel, ChatRequest};
use langgraph_core::messages::Message;
use serde::{Deserialize, Serialize};
use std::sync::Arc;
use tracing::{debug, info};

/// Prompt template for pattern planning
const PATTERN_PLANNING_PROMPT: &str = r#"You are an AI planning assistant that generates execution plans for patterns.

Pattern: {pattern_name}
Description: {pattern_description}

User Intent: {user_intent}

Context: {context}

Generate a concrete execution plan as a JSON array of steps. Each step should have:
- "name": A descriptive name for the step
- "action": The specific action to take
- "dependencies": Array of step names this depends on (empty if none)

Example format:
[
  {"name": "analyze_requirements", "action": "Analyze user requirements", "dependencies": []},
  {"name": "design_solution", "action": "Design solution architecture", "dependencies": ["analyze_requirements"]},
  {"name": "implement", "action": "Implement the solution", "dependencies": ["design_solution"]}
]

Generate the execution plan:"#;

/// Execution plan generated by LLM
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExecutionPlan {
    /// Pattern ID this plan is for
    pub pattern_id: String,
    /// List of execution steps
    pub steps: Vec<PlanStep>,
    /// Estimated total steps
    pub estimated_steps: usize,
}

/// A single step in an execution plan
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PlanStep {
    /// Step name
    pub name: String,
    /// Action to take
    pub action: String,
    /// Dependencies (names of steps that must complete first)
    #[serde(default)]
    pub dependencies: Vec<String>,
}

/// LLM-based pattern planner
pub struct LlmPatternPlanner {
    /// Planning LLM client
    llm: Arc<dyn ChatModel>,
}

impl LlmPatternPlanner {
    /// Create a new LLM pattern planner
    ///
    /// # Arguments
    /// * `llm` - Planning LLM client
    pub fn new(llm: Arc<dyn ChatModel>) -> Self {
        Self { llm }
    }

    /// Generate an execution plan for a pattern
    ///
    /// # Arguments
    /// * `pattern_id` - Pattern identifier
    /// * `pattern_description` - Description of the pattern
    /// * `user_intent` - User's intent/request
    /// * `context` - Additional context for planning
    ///
    /// # Returns
    /// * Generated execution plan
    pub async fn generate_plan(
        &self,
        pattern_id: impl Into<String>,
        pattern_description: impl Into<String>,
        user_intent: impl Into<String>,
        context: &serde_json::Value,
    ) -> Result<ExecutionPlan> {
        let pattern_id = pattern_id.into();
        let pattern_description = pattern_description.into();
        let user_intent = user_intent.into();

        info!("Generating execution plan for pattern: {}", pattern_id);

        // Build context description
        let context_desc = if context.is_null() || context == &serde_json::json!({}) {
            "None".to_string()
        } else {
            serde_json::to_string_pretty(context)
                .unwrap_or_else(|_| "Unable to serialize context".to_string())
        };

        // Create prompt
        let prompt = PATTERN_PLANNING_PROMPT
            .replace("{pattern_name}", &pattern_id)
            .replace("{pattern_description}", &pattern_description)
            .replace("{user_intent}", &user_intent)
            .replace("{context}", &context_desc);

        // Call LLM
        let messages = vec![Message::human(prompt)];
        let request = ChatRequest::new(messages);

        let response = self.llm.chat(request).await.map_err(|e| {
            OrchestratorError::General(format!("LLM planning failed: {}", e))
        })?;

        // Parse response
        let response_text = response.message.text().unwrap_or("");
        let steps = self.parse_plan_response(response_text)?;

        // Validate plan
        self.validate_plan(&steps)?;

        let plan = ExecutionPlan {
            pattern_id,
            steps: steps.clone(),
            estimated_steps: steps.len(),
        };

        debug!("Generated plan with {} steps", plan.estimated_steps);

        Ok(plan)
    }

    /// Parse LLM response into plan steps
    fn parse_plan_response(&self, response: &str) -> Result<Vec<PlanStep>> {
        // Try to extract JSON array from response
        let json_str = self.extract_json_array(response)?;

        // Parse JSON
        let steps: Vec<PlanStep> = serde_json::from_str(&json_str).map_err(|e| {
            OrchestratorError::General(format!("Failed to parse plan JSON: {}", e))
        })?;

        if steps.is_empty() {
            return Err(OrchestratorError::General(
                "LLM returned empty execution plan".to_string(),
            ));
        }

        Ok(steps)
    }

    /// Extract JSON array from text (handle markdown code blocks)
    fn extract_json_array(&self, text: &str) -> Result<String> {
        // Try to find JSON in markdown code block
        if let Some(start) = text.find("```json") {
            if let Some(end) = text[start..].find("```") {
                let json_content = &text[start + 7..start + end].trim();
                return Ok(json_content.to_string());
            }
        }

        // Try to find JSON in plain code block
        if let Some(start) = text.find("```") {
            if let Some(end) = text[start + 3..].find("```") {
                let json_content = &text[start + 3..start + 3 + end].trim();
                if json_content.starts_with('[') {
                    return Ok(json_content.to_string());
                }
            }
        }

        // Try to find JSON array directly
        if let Some(start) = text.find('[') {
            if let Some(end) = text.rfind(']') {
                if end > start {
                    let json_content = &text[start..=end];
                    return Ok(json_content.to_string());
                }
            }
        }

        Err(OrchestratorError::General(
            "Could not extract JSON array from LLM response".to_string(),
        ))
    }

    /// Validate execution plan
    fn validate_plan(&self, steps: &[PlanStep]) -> Result<()> {
        // Check for cycles in dependencies
        for step in steps {
            self.check_dependencies(step, steps)?;
        }

        // Check that all dependency references are valid
        let step_names: Vec<&str> = steps.iter().map(|s| s.name.as_str()).collect();
        for step in steps {
            for dep in &step.dependencies {
                if !step_names.contains(&dep.as_str()) {
                    return Err(OrchestratorError::General(format!(
                        "Invalid dependency '{}' in step '{}'",
                        dep, step.name
                    )));
                }
            }
        }

        Ok(())
    }

    /// Check for circular dependencies
    fn check_dependencies(&self, step: &PlanStep, all_steps: &[PlanStep]) -> Result<()> {
        let mut visited = std::collections::HashSet::new();
        self.check_cycle(step, all_steps, &mut visited)?;
        Ok(())
    }

    /// Recursive cycle detection
    fn check_cycle(
        &self,
        step: &PlanStep,
        all_steps: &[PlanStep],
        visited: &mut std::collections::HashSet<String>,
    ) -> Result<()> {
        if visited.contains(&step.name) {
            return Err(OrchestratorError::General(format!(
                "Circular dependency detected involving step '{}'",
                step.name
            )));
        }

        visited.insert(step.name.clone());

        for dep_name in &step.dependencies {
            if let Some(dep_step) = all_steps.iter().find(|s| &s.name == dep_name) {
                self.check_cycle(dep_step, all_steps, visited)?;
            }
        }

        visited.remove(&step.name);
        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use async_trait::async_trait;
    use langgraph_core::error::{GraphError, Result as GraphResult};
    use langgraph_core::llm::{self, ChatResponse};

    // Mock ChatModel for testing
    #[derive(Clone)]
    struct MockChatModel {
        response: String,
    }

    #[async_trait]
    impl ChatModel for MockChatModel {
        async fn chat(&self, _request: ChatRequest) -> GraphResult<ChatResponse> {
            Ok(ChatResponse {
                message: Message::ai(self.response.clone()),
                reasoning: None,
                usage: None,
                metadata: Default::default(),
            })
        }

        async fn stream(&self, _request: ChatRequest) -> GraphResult<llm::ChatStreamResponse> {
            Err(GraphError::Validation("Stream not implemented for mock".to_string()))
        }

        fn clone_box(&self) -> Box<dyn ChatModel> {
            Box::new(self.clone())
        }
    }

    #[tokio::test]
    async fn test_llm_pattern_planner() {
        let mock_response = r#"
```json
[
  {"name": "step1", "action": "First action", "dependencies": []},
  {"name": "step2", "action": "Second action", "dependencies": ["step1"]}
]
```
"#;

        let mock_llm = Arc::new(MockChatModel {
            response: mock_response.to_string(),
        });

        let planner = LlmPatternPlanner::new(mock_llm);
        let plan = planner
            .generate_plan(
                "test_pattern",
                "Test pattern description",
                "User wants to test",
                &serde_json::json!({}),
            )
            .await
            .unwrap();

        assert_eq!(plan.steps.len(), 2);
        assert_eq!(plan.steps[0].name, "step1");
        assert_eq!(plan.steps[1].dependencies, vec!["step1"]);
    }

    #[test]
    fn test_extract_json_array() {
        let planner = LlmPatternPlanner::new(Arc::new(MockChatModel {
            response: "".to_string(),
        }));

        // Test markdown code block
        let text1 = "```json\n[{\"name\": \"test\"}]\n```";
        let result1 = planner.extract_json_array(text1).unwrap();
        assert!(result1.contains("name"));

        // Test direct JSON
        let text2 = "[{\"name\": \"test\"}]";
        let result2 = planner.extract_json_array(text2).unwrap();
        assert!(result2.contains("name"));
    }

    #[test]
    fn test_validate_plan() {
        let planner = LlmPatternPlanner::new(Arc::new(MockChatModel {
            response: "".to_string(),
        }));

        // Valid plan
        let valid_steps = vec![
            PlanStep {
                name: "step1".to_string(),
                action: "Action 1".to_string(),
                dependencies: vec![],
            },
            PlanStep {
                name: "step2".to_string(),
                action: "Action 2".to_string(),
                dependencies: vec!["step1".to_string()],
            },
        ];
        assert!(planner.validate_plan(&valid_steps).is_ok());

        // Invalid plan (missing dependency)
        let invalid_steps = vec![PlanStep {
            name: "step1".to_string(),
            action: "Action 1".to_string(),
            dependencies: vec!["nonexistent".to_string()],
        }];
        assert!(planner.validate_plan(&invalid_steps).is_err());
    }
}

